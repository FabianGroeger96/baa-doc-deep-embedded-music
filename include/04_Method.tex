\chapter{Method}
\label{ch:Method}
In this chapter, the methods, which are used in the thesis, will be described. This includes the project plan (\ref{sec:Project-Plan}), where the whole project plan is explained in detail, the procedure model (\ref{sec:Procedure-Model}), which is used to realise the thesis and the risk analysis (\ref{sec:Risk-Analysis}). This chapter further describes how the success of the project is measured and compared (\ref{sec:Evaluation}) as well as how to test the individual components (\ref{sec:Testing}). Finally, it contains information about the overall project structure (\ref{sec:Project-Structure}).

\section{Project plan}
\label{sec:Project-Plan}
Within this section of the thesis, the project plan is shown, based on figure \ref{fig:Project-Plan}. The project is divided into four different phases. At the end of each phase, there is a milestone, which will review the process within the phase and provide an outlook on the next phase. The first phase is doing research, where many resources are gathered, and essential pieces of information are extracted and documented. The goal of this stage is to get a better understanding of the concepts, which already exist. It further aims to get insights into the current status of the research in the audio domain. Afterwards, the realisation of the project takes place, where the models, input pipeline and Tile2Vec will be implemented and tested. Then different experiments will be conducted, to validate the realisation and experiment with different hyperparameters. At last, the documentation will be finalised and proofread. There is also a one week buffer before the finalisation of the documentation, which will be used for unpredictable problems. The chapters of the documentation will be continuously written during each phase.
\newline
\newline
The figure \ref{fig:Project-Plan} illustrates the project plan, where the phases are shown in orange, tasks within each phase are blue, and milestones are red diamonds. Each dotted column represents one week within a specific month (e.g. Feb, Mar). A task further has information about how far it is into completion.

% set page layout to portrait and save layout for later
\storeareas\defaultpagestyle
\KOMAoptions{pagesize,paper=landscape,DIV=20}
\storeareas\landscapevalues

\begin{figure}
\centering
     \begin{ganttchart}[%Specs
     x unit = 0.9cm,  %<---------------------- New x unit 
     y unit title=0.4cm,
     y unit chart=0.5cm,
     vgrid, hgrid,
     title height=1,
     title label font=\bfseries\footnotesize,
     progress=today,
     today=14,
     today label=Current Week,
     bar/.append style={fill=blue!50},
     group/.append style={draw=black, fill=orange!40!white},
     milestone/.append style={draw=black, fill=red!40!white, xscale=0.55}, % 0.5/0.9 â‰ˆ 0.5555
     milestone progress label node/.append style={right=0.2},
     bar incomplete/.append style={fill=none},
     group incomplete/.append style={draw=black,fill=none},
     bar height=0.7,
     group right shift=0,
     group top shift=0.5,
     group height=.3,
     group peaks width={0.2},
     inline]{1}{16}
    %labels

    %\gantttitle{bachelor thesis - deep embedded music}{18}\\
    \gantttitle[]{2020}{16} \\                
    \gantttitle{Feb}{2}
    \gantttitle{Mar}{4}
    \gantttitle{Apr}{4}
    \gantttitle{May}{4}
    \gantttitle{Jun}{2}\\
    
    % 1. Phase: Research
    \ganttgroup[inline=false]{Research}{1}{4}\\ 
    \ganttbar[progress=100,inline=false]{Dataset}{1}{1}\\
    \ganttbar[progress=100,inline=false]{Audio processing}{1}{1}\\
    \ganttbar[progress=100,inline=false]{Triplet loss}{2}{2}\\
    \ganttbar[progress=100,inline=false]{Tile2Vec}{3}{3}\\
    \ganttbar[progress=100,inline=false]{Evaluate research}{4}{4}\\
    \ganttmilestone[inline=false]{Research finished}{4} \\ % M1
    
    % 2. Phase: Realization
    \ganttgroup[inline=false]{Realisation}{5}{8} \\
    \ganttbar[progress=100,inline=false]{Project setup}{5}{5} \\
    \ganttbar[progress=100,inline=false]{Input pipeline}{5}{5} \\
    \ganttbar[progress=100,inline=false]{Default model architectures}{6}{6} \\
    \ganttbar[progress=100,inline=false]{Evaluation workflow}{6}{6} \\
    \ganttbar[progress=100,inline=false]{Unit tests}{6}{6} \\
    \ganttmilestone[inline=false]{Project Setup finished}{6} \\ % M2
    \ganttbar[progress=100,inline=false]{Tile2Vec implementation}{7}{7} \\
    \ganttbar[progress=100,inline=false]{Architecture for experiments}{8}{8} \\
    \ganttmilestone[inline=false]{Realisation finished}{8} \\ % M3
    
    \ganttmilestone[inline=false]{Interim presentation}{9} \\ % M3
    
    % 3. Phase: Experiments
    \ganttgroup[inline=false]{Experiments}{9}{12} \\
    \ganttbar[progress=100,inline=false]{Conduct experiments}{9}{9} \\
    \ganttbar[progress=100,inline=false]{Validate embeddings}{10}{11} \\
    \ganttmilestone[inline=false]{Experiments finished}{11} \\ % M4
    \ganttbar[progress=100,inline=false]{Visualise embeddings}{12}{12} \\
    
    % Buffer
    \ganttgroup[inline=false]{Buffer}{13}{13} \\
    
    % 4. Phase: Documentation
    \ganttgroup[inline=false]{Finalise documentation}{14}{16} \\
    \ganttbar[progress=50,inline=false]{Create pitch video and web abstract}{14}{14} \\
    \ganttbar[progress=100,inline=false]{Finalise documentation}{14}{15} \\
    \ganttbar[progress=20,inline=false]{Proofread documentation}{16}{16} \\
    \ganttmilestone[inline=false]{Thesis submission}{16}
\end{ganttchart}
\caption{Project plan}
\label{fig:Project-Plan}
\end{figure}

% set page back to portrait
\clearpage
\defaultpagestyle

\section{Procedure model}
\label{sec:Procedure-Model}
The waterfall model is a breakdown of project activities into linear sequential phases, where each phase depends on the deliverables of the previous one and corresponds to a specialisation of tasks. For this project, a custom waterfall model is used, which differs quite profoundly from Royce's original waterfall model. The procedure model is shown in figure \ref{fig:Project-Plan}.
\newline
\newline
The main reason why a linear procedure model is used and not an iterative such as SCRUM is, that, the project is an innovation/research project, where the main focus is on conducting experiments and searching for new findings for the problem definition. Therefore, it is obvious which tasks need to be completed during each phase and what each one of them has to deliver before going to the next phase. Another reason why the waterfall model is chosen is that the project is done only by a single person and therefore there is no waiting for others to complete their task before going to the next phase since all of the work carried out is done by one person. Another benefit compared to an iterative procedure model is, that the project is already clearly defined by the start of the project and the artefacts, which will need to be delivered by the end of the project, is as well given at the start. Therefore the status of the project is evident at any point during the project.

\subsection{Project phases}
\label{sec:Project-Phases}
\begin{table}[htb]
    \centering
    \caption{Phases in the project plan}
	\label{tab:Phases}
    \begin{tabular}{p{.05\textwidth} | p{.15\textwidth} | p{.70\textwidth}}
        \toprule
        \textbf{\#} & \textbf{Phase} & \textbf{Description} \\ 
        \midrule[1pt]
        P1 & Research & The first phase of the project is to do research on the field and to describe it in chapter \ref{ch:Related-Work}. The deliverable of this phase is the finished draft of the chapter related work (D1). \\
        \hline
        P2 & Realisation & The second phase is relatively large and is used to implement the codebase of the project, which includes the code for training, testing and evaluating. The deliverable of this phase is a fully functional and tested codebase to train and evaluate embedding models (D2 \& D3).\\
        \hline
        P3 & Experiments & In the third phase, all of the experiments will be conducted and evaluated. Within this phase, there is the possibility that the codebase will be adjusted to optimise the performance of the experiments. The deliverables of this phase are two fully trained models, one for the noise detection dataset and one for the music dataset (D4).\\
        \hline
        P4 & Buffer & The buffer phase is used to provide an extra one week buffer if there is a  delay with one of the phases before. Therefore, this phase is variable and does not have clear deliverables at the start of the project. If the buffer is not needed, it will be used to extend the last phase P5. \\
        \hline
        P5 & Finalise documentation & The last phase is used to finalise the thesis, create the web abstract and the pitching video. These are all artefacts, which are needed to submit the thesis. The deliverables are, therefore, the finished documentation, the pitching video and the web abstract (D5). \\
        \bottomrule
    \end{tabular}
\end{table}
\noindent
The project is grouped into four phases, \textit{research}, \textit{realisation}, \textit{experiments} and \textit{finalise documentation}. However, there is as well an additional \textit{buffer} phase which will be used where ever there is a need for an additional week to finish the phase. All of these phases consist of multiple tasks which need to be completed before going to the next step. Each phase is only then completed when the deliverables for the phase are delivered and reviewed. The table \ref{tab:Phases}, describes each phase in more detail and further describes the deliverables of each one of them.

\subsection{Deliverables}
\label{sec:Deliverables-Project}
\begin{table}[htb]
    \centering
    \caption{Artefacts to deliver}
	\label{tab:Deliverables}
    \begin{tabular}{p{.05\textwidth} | p{.25\textwidth} | p{.60\textwidth}}
        \toprule
        \textbf{\#} & \textbf{Deliverable} & \textbf{Description} \\ 
        \midrule[1pt]
        D1 & Research \gls{DSP}, \gls{NN} and state of the art (related work) & These artefacts need to be delivered in the form of chapter \ref{ch:Related-Work} (related work), which needs to be reviewed and accepted by the advisor. \\
        \hline
        D2 & Evaluation workflow &  Implementation of the evaluation workflow to validate the model performance on either of the datasets. \\
        \hline
        D3 & Implementation codebase & Implementation of the codebase for training the model. This consists of the realisation of the input and training pipeline. \\
        \hline
        D4 & Experiments (study docs) &  For each of the conducted experiments, a separate study doc needs to be delivered, which will be used to provide detailed information about the experiments and its outcome. \\
        \hline
        D5 & Documentation & The documentation needs to be delivered at the end of the project along with a web abstract and a pitching video, which will be needed to complete the upload of the thesis. \\
        \bottomrule
    \end{tabular}
\end{table}
\noindent
Deliverables are artefacts which need to be delivered at the end of each phase to complete it. They will be examined and reviewed before going to the next phase. All of the deliverables in the project are shown in table \ref{tab:Deliverables}. Each deliverable corresponds to a particular phase, and their affiliation is shown in table \ref{tab:Phases}.

\subsection{Milestones}
\label{sec:Milestones-Project}
\begin{table}[htb]
    \centering
    \caption{Milestones overview along with their corresponding date}
	\label{tab:Milestones}
    \begin{tabular}{p{.05\textwidth} | p{.45\textwidth} | p{.20\textwidth} | p{.15\textwidth}}
        \toprule
        \textbf{\#} & \textbf{Milestone} & \textbf{Deliverables} & \textbf{Date} \\ 
        \midrule[1pt]
        M0 & Project start & - & 17.02.2020\\
        \hline
        M1 & Research finished & D1 & 15.03.2020\\
        \hline
        M2 & Project setup finished & D2 & 29.03.2020\\
        \hline
        M3 & Realisation finished & D3 & 12.04.2020\\
        \hline
        M4 & Interim presentation & - & 21.04.2020\\
        \hline
        M5 & Experiments finished & D4 & 03.05.2020\\
        \hline
        M6 & Thesis submission & D5 & 05.06.2020\\
        \bottomrule
    \end{tabular}
\end{table}
\noindent
There is a specific milestone at the end of each phase \ref{tab:Phases}, which reviews the deliverables and the work done within the phase. The milestone review further aims to provide an outlook on the next phase and the current status of the project. The review is done by writing a report for each milestone, which can be found in the appendix milestone reports (\ref{app:Milestone-Reports}). Each of these reports gives insight about the status of the project by answering the questions \textit{what was done since the last reporting?}, \textit{what is the state of progress of the project} and \textit{what are the top three risks?}, which further includes the planned measures to take. The reports provide valuable insights into the projects current status and are therefore used to plan the next phases. Table \ref{tab:Milestones} shows all the milestones with the corresponding data assigned to it. The milestones are illustrated in the full project plan \ref{fig:Project-Plan} in red.
\newline
\newline
It is important to note that the milestone \textit{M4: Interim presentation} does not conclude a phase. It is purely used to show wherein the project the interim presentation is held and what needs to be delivered until then. Therefore, for this milestone, no milestone report is being written.

% set page layout to landscape
\clearpage
\landscapevalues

\section{Risk analysis}
\label{sec:Risk-Analysis}
\begin{table}[htb]
    \centering
    \caption{Risk analysis}
	\label{tab:Risk-Analysis}
    \begin{tabular}{p{.05\textwidth} | p{.30\textwidth} | p{.30\textwidth} | p{.30\textwidth}}
        \toprule
        \textbf{\#} & \textbf{Risk} & \textbf{Consequences} & \textbf{Planned measurements} \\ 
        \midrule[1pt]
        R1 & The codebase has flaws in it. & The experiments fail, the model does not train, and the codebase needs to be changed during a late stage. & Using a test-first approach with automated unit tests. \\
        \hline
        R2 & The model fails to find an underlying structure of the data. & The project fails in a late-stage, and all of the conducted experiments showed that the model failed. & Implementing small models and train them in an early stage to show the possibility of solving the problem.  \\
        \hline
        R3 & The training of the model fails because there is not enough computing power. & The large models needed to find an optimal embedding space can not be trained. & Early testing on the available computing resources and if more needed, rent \gls{GPU} instances on \texttt{vast.ai}. \\
        \hline
        R4 & Optimal hyperparameters for the model can not be found. & An optimal embedding space can not be found, and all of the experiments result in suboptimal results. & The first results and hyperparameters are discussed with the advisor, and thereafter the advisor is contacted if help is needed. \\
        \hline
        R5 & The model fails to find an optimal embedding space for the music dataset & The second part of the project fails, and the unsupervised triplet loss approach will be discarded. & This is a limitation of the project and is part of the research process. \\
        \hline
        R6 & The project can not be completed due to time limitations. & Some of the deliverables can not be delivered at the end of the project. The goal of the project is not reached. & Early start of the documentation, consequent documenting, meetings with the advisor and rapid changes if a loss of time is observed. \\ 
        \bottomrule
    \end{tabular}
\end{table}
\noindent
This section shows the risk analysis done to show the possible risks when conducting the project. Table \ref{tab:Risk-Analysis} shows the risk analysis, which consists of the risk, the possible consequences and the planned measurements. The table \ref{tab:Risk-Matrix} shows the residual risk matrix, which provides an overview of the possibility of the occurrence of each risk. For all of the risks, the measurements are planned to get them as low as reasonably possible. Therefore the goal is to have all of the risks in a green section, which indicates that it is acceptable. Yellow represents acceptable. However, further risk reduction should be made. Furthermore, red indicates not acceptable, and a risk reduction is profoundly needed. The risk matrix \ref{tab:Risk-Matrix} show the probability of the occurrence of each risk when taking the counter measurements already into account.

% set page back to portrait
\clearpage
\defaultpagestyle

\begin{table}[htb]
\centering
\scriptsize
\caption{Residual risk matrix}
\label{tab:Risk-Matrix}
\begin{tabular}{|p{3cm}|p{2cm}|p{1.6cm}| p{1.8cm} |p{1.8cm}| p{1.6cm}|}
    \hline \diagbox[innerwidth=3cm]{Frequency}{Consequence} & 1-Very Unlikely & 2-Remote & 3-Occasional & 4-Probable & 5-Frequent\\ [10pt]
    \hline 4-Catastrophic & \cellcolor{yellow!40!white} & \cellcolor{red!40!white} & \cellcolor{red!40!white} & \cellcolor{red!40!white} &\cellcolor{red!40!white} \\ [10pt]
    \hline 3-Critical & R2 \cellcolor{green!40!white} & R6 \cellcolor{yellow!40!white} & R5\cellcolor{yellow!40!white} & \cellcolor{red!40!white} &\cellcolor{red!40!white} \\ [10pt]
    \hline 2-Major & \cellcolor{green!40!white} & R3 \cellcolor{green!40!white} & R4 \cellcolor{yellow!40!white} & R1 \cellcolor{yellow!40!white} &\cellcolor{red!40!white} \\ [10pt]
    \hline 1-Minor & \cellcolor{green!40!white} & \cellcolor{green!40!white} & \cellcolor{green!40!white} &\cellcolor{yellow!40!white} &\cellcolor{yellow!40!white} \\ [10pt]
    \hline
\end{tabular}
\end{table}

\section{Evaluation}
\label{sec:Evaluation}
The evaluation is done for both used datasets separately, and in a relatively different way, therefore both evaluations are described in different subsections, the evaluation of the DCASE dataset (\ref{sub:Eval-DCASE}) and the evaluation of the music dataset (\ref{sub:Eval-Music}).

\subsection{DCASE 2018 - Task 5 dataset}
\label{sub:Eval-DCASE}
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[standard/.style={inner sep=0pt,align=center,draw,text height=1.25em,text depth=0.5em},
    decoration={brace}]
    \node[text width=8cm, yshift=1cm, standard] (Trd)  {development dataset};
    \node[right=0.5em of Trd, standard, text width=4cm] (Ted)  {evaluation dataset};
    \node[fit=(Trd)(Ted), yshift=1cm, standard] (Ald)  {all data from the DCASE challenge}; 
    \node[anchor=north west, standard, text width=5.8cm, fill=yellow!30!white, yshift=-0.3cm] at (Trd.south-|Trd.west) (dev) {train};
    \node[anchor=north west, standard, text width=2cm, fill=orange!30!white, right=0.5em of dev](eval) {eval};
    \node[anchor=north west, standard, text width=4cm, fill=red!30!white, yshift=-0.3cm] at (Ted.south-|Ted.west) (Ted2) {test};
    \end{tikzpicture}
\caption{Overview of the dataset split for the DCASE dataset}
\label{fig:DCASE-split}
\end{figure}
\noindent
The evaluation of the embedding space for the \fullref{sub:DCASE-Task-Dataset} is done in a few separate steps. The development dataset of the challenge is further split into an \texttt{development-training} and an \texttt{development-evaluation} set. First, an arbitrary embedding architecture is being trained on the \texttt{development-training} dataset and evaluated on the \texttt{development-evaluation} set using the metrics available for the embedding (\ref{sec:Metrics}), this process is repeated until an architecture with the desired performance is found.
\newline
\newline
To then further evaluate the performance of the resulting embedding space, a simple classifier is trained using the embeddings as input. There are two different classifiers used to evaluate the embedding space, a linear logistic classifier and a two-layered classifier. Both of the classifiers are visualised in figure \ref{fig:Classifier-DCASE-Visualisation}.
\newline
\newline
The linear logistic classifier (\ref{fig:logistic-classifier}) only consists of a single softmax output layer, which has the same amount of output units as there are classes in the dataset. The two-layered classifier (\ref{fig:dense-classifier}) consists of a two-layered dense model and a softmax output layer. Both classifiers are used to evaluate the embedding space. However, their idea is quite different from each other. The linear classifier tries to separate the embedding space using a hyperplane and therefore shows how well separated the embedding clusters are. The dense classifier shows how much of a performance gain there is when using the embedding space with a deeper \gls{NN}.
\newline
\newline
The macro-averaged F1 score of the linear logistic classifier is compared to other F1 scores from the experiment, to evaluate the different embedding spaces. The classifier is trained on the raw audio representation itself to provide a baseline F1 score, to show the performance gain when using the embedding space.
\newline
\newline
In the final step, the resulting embedding space, as well as the trained classifier will be used on the evaluation dataset of the challenge, where the resulting macro-averaged F1 score is compared to the accomplished score by the submitted models. The aim is to show the result which can be accomplished when using the dataset in an unsupervised setting and only focusing on the audio data itself rather than the label.
\newline
\newline
The overall goal is to achieve the highest possible macro-averaged F1 score on the evaluation dataset provided by the DCASE challenge. 
\newline
\newline
However, the main goal is to show that the resulting embedding space represents a specific structure of the dataset and therefore represents meaning. Since the embedding space only focuses on the audio stream itself, the optimal space clusters audio segments which sound similar in the nearby region irrelevant of the label. This evaluation is done manually by examining the embedding space.
\begin{figure}[htbp]
\centering
\begin{subfigure}{.5\linewidth}
  \centering
  \begin{tikzpicture}[start chain=going below, node distance=15pt,
        point/.append style={on chain, join=by {signal}},
        layer/.append style={on chain, join=by {signal}}]
        \node[point] {Input to classifier, embedded sample};
        \node[conv] {Dense layer (hidden layer 1): \\ 256 units, ReLU};
        \node[conv] {Dense layer (hidden layer 2): \\ 256 units, ReLU};
        \node[activation] {Dense output: \\ 9 units (num. of classes), softmax};
        \node[point] {Output};
    \end{tikzpicture}
  \caption{Dense classifier}
  \label{fig:dense-classifier}
\end{subfigure}%
\begin{subfigure}{.5\linewidth}
  \centering
  \begin{tikzpicture}[start chain=going below, node distance=15pt,
        point/.append style={on chain, join=by {signal}},
        layer/.append style={on chain, join=by {signal}}]
        \node[point] {Input to classifier, embedded sample};
        \node[activation] {Dense output: \\ 9 units (num. of classes), softmax};
        \node[point] {Output};
    \end{tikzpicture}
  \caption{Linear logistic classifier}
  \label{fig:logistic-classifier}
\end{subfigure}
\caption{Visualisation of the different classifier architectures}
\label{fig:Classifier-DCASE-Visualisation}
\end{figure}

\subsection{Music dataset}
\label{sub:Eval-Music}
The process of evaluating the embedding space for the music dataset (\ref{sub:Music-Dataset}) is a bit more complicated since there are no prior results on this exact dataset, nor a baseline model to compare it to. The first evaluation of the embedding is relatively similar to the one for the \fullref{sub:Eval-DCASE}. The dataset is split into an \texttt{training}, \texttt{evaluation} and \texttt{test} set. Then the \texttt{training} dataset is used to train the embedding architecture and is evaluated on the \texttt{evaluation} set with the metrics specified in metrics (\ref{sec:Metrics}). After that, the model, which resulted in the best metrics, is chosen and is further evaluated.
\newline
\newline
The idea of training a simple linear logistic classifier using the embedding space as input, which was specifically made for the comparison of the noise detection dataset, can however as well be used when evaluating the music dataset. The result of the classifier is then not used to compare the results to some other results, but to check how easy a linear classifier can separate the clusters, by trying to place a hyperplane between them. This result then gives some insights about the resulting clusters and their separation.
\newline
\newline
Since one of the requirements of the project is to examine the resulting clusters of the music embedding space, a simple clustering algorithm, such as \fullref{sub:K-Means}, is being applied on the resulting embeddings of the \texttt{test} set and afterwards on the full dataset. This should provide a crucial insight into the resulting clusters and the performance of the embedding architecture. It further aims to show which categories of the dataset are in the nearby region and therefore should represent similar categories. 
\newline
\newline
Another requirement of the embedding space is that it should provide some similarity measure for the embedded songs. Therefore the resulting clusters, as well as their distances between each other, are essential properties which will need to be evaluated. Since the examination of these distances as well as the similarity between each music genre is quite hard, mister Emanuel Oehri, who provided the music dataset (\ref{sub:Music-Dataset}), helps to examine the distances between the clusters and gives feedback about their similarity from his professional opinion. He will further examine the resulting similarity between two songs of different genres which are projected into their nearby region, as well as the similarity between segments which have an inconsistent neighbourhood. The evaluation with mister Emanuel Oehri will be structured as an interview, where the interview guide can be found in the appendix \ref{sec:Interview-Guide}. The results of the conducted qualitative analysis are then located in section \ref{sub:Results-Music-Qualitative-analysis}.

\section{Testing}
\label{sec:Testing}
All of the components in the project are tested with the TensorFlow unit tests module called \texttt{tf.test}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/test}}. A test environment is created, which contains a small fraction of the dataset to speed up the testing process because testing on the entire dataset would be infeasible. For the entire implementation process, the test first principle is used to produce better and more reliable code. All test cases are kept as small as possible. However, some require other dependencies, such as the input pipeline to work. Therefore some tests are quite big because they need a lot of different dependencies. However, since all of the components are tested as well, and the thesis is a research project, it is plausible.
\newline
\newline
Table \ref{tab:Components-Testing} lists all of the components in the project and gives detailed information about the testing method and the reason behind it. All of the unit tests can be found in the source repository of this project in the \texttt{test} directory, categorised by the component they test. Since some of the tests are conducted manually, a comprehensive test concept can be found in the appendix (\ref{tab:Test-Concept}) for the manual tests.
\begin{table}[htbp]
    \centering
    \caption{Testing method of each project component}
	\label{tab:Components-Testing}
    \begin{tabular}{p{.15\textwidth} | p{.20\textwidth} | p{.55\textwidth}}
        \toprule
        \textbf{Component} & \textbf{Testing method} & \textbf{Reason} \\ 
        \midrule[1pt]
        \texttt{dataset} & unit test & result can be validated, since result is deterministic \\
        \hline
        \texttt{feature\_extractor} & unit test & result can be validated, since result is deterministic \\
        \hline
        \texttt{input\_pipeline} & unit test & result can be validated, since result is deterministic \\
        \hline
        \texttt{loss}  & unit test & result can be validated, since result is deterministic \\
        \hline
        \texttt{models\_classifier} & unit test + manual testing & automated tests check if the model can be built and outputs values if the model trains can only be checked manually by observing the metrics \\
        \hline
        \texttt{models\_embedding} & unit test + manual testing & automated tests check if the model can be built and outputs values if the model trains can only be checked manually by observing the metrics \\
        \hline
        \texttt{training} & manual testing & result is not deterministic and can therefore only be tested manually by looking at the metrics over the training process \\
        \hline
        \texttt{utils} & unit test & result can be validated, since result is deterministic \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Project structure}
\label{sec:Project-Structure}
The thesis consists of two projects, a project for the documentation and a project for the source of the thesis. Both projects are git repositories hosted on GitHub\footnote{\url{https://github.com/}}. For the duration of the thesis, both repositories are kept private, and after the completion of the project will be open-sourced.

\begin{figure}[ht]
    \dirtree{%
    .1 deep-embedded-music/.
    .2 data/.
    .2 experiments/.
    .3 config/.
    .3 DCASE/.
    .3 DJ/.
    .2 notebooks/.
    .2 src/.
    .2 test/.
    .2 test-environment/.
    .2 Dockerfile.
    .2 onstart.sh.
    .2 requirements.txt.
    }
\caption{Overview of the source repository \flqq deep embedded music\frqq}
\label{fig:Project-Overview-Source}
\end{figure}
\noindent
Figure \ref{fig:Project-Overview-Source} illustrates the project structure of the source repository, which can be found on GitHub\footnote{\url{https://github.com/FabianGroeger96/deep-embedded-music}}. The illustration shows an overview of the overall structure of the repository. More in-depth explanation can be found in section \ref{ch:Realisation}.

\begin{figure}[ht]
    \dirtree{%
    .1 baa-doc-deep-embedded-music/.
    .2 files/.
    .2 img/.
    .2 include/.
    .2 intermediate-presentation/.
    .2 final-presentation/.
    .2 study-doc/.
    .2 BAA-Documentation.tex.
    .2 README.md.
    .2 references.bib.
    }
\caption{Overview of the documentation repository \flqq baa-doc-deep-embedded-music\frqq}
\label{fig:Project-Overview-Documentation}
\end{figure}
\noindent
Figure \ref{fig:Project-Overview-Documentation} illustrates the project structure of the repository used for the documentation, which can be found on GitHub\footnote{\url{https://github.com/FabianGroeger96/baa-doc-deep-embedded-music}}. The repository contains a folder for the \textit{documentation}, \textit{interim presentation}, \textit{final presentation} and the \textit{study doc}. The study doc contains detailed information about the conducted experiments, which are attached in the appendix study doc (\ref{app:Study-Doc}).
\newline
\newline
Both datasets used for the thesis were used as they are and therefore did not need to be kept within a specific repository for the project. The dataset of the DCASE challenge 2018 - Task 5 consists of a development and evaluation dataset, which are both hosted on Zendo\footnote{\url{https://zenodo.org}}. The development dataset is available under \url{https://zenodo.org/record/1247102} while the evaluation dataset is available under \url{https://zenodo.org/record/1964758}. 
\newline
\newline
The music dataset was kindly provided by Mr Emanuel Oehri and is available as a private GitLab Git LFS repository, which will be kept as a private repository since all the songs provided are property of Mr Oehri.

\section{Resources}
\label{sec:Resources}
Deep neural networks architectures are computationally expensive what results in long-running experiments. Using powerful \glspl{GPU} reduces the processing time up to a factor of 20. The Lucerne University of Applied Science and Arts supported this thesis with a GPU GTX 1080Ti, 11 GB RAM, hosted within the enterprise lab\footnote{\url{https://www.enterpriselab.ch/}}. 
\newline
\newline
Additionally, \glspl{GPU} were rented on vast.ai\footnote{\url{https://vast.ai/}} during specific experiments to decrease the training time, by running experiments in parallel. A Tesla V100 \gls{GPU} 16 GB RAM was mostly rented to mitigate the problem of not having enough RAM on the GPU for experiments with large models (e.g. such as ResNet50).

