\chapter{Method}
\label{ch:Method}
In this chapter, the used methods within the thesis will be described. This includes the project plan (\ref{sec:Project-Plan}), where the whole project plan is explained in detail, and the procedure model (\ref{sec:Procedure-Model}), which is used to realise the thesis. This chapter also contains the methods and structure on how to test (\ref{sec:Testing}) and evaluate (\ref{sec:Evaluation}) the success of the project, as well as the overall project structure (\ref{sec:Project-Structure}).

\section{Project plan}
\label{sec:Project-Plan}
Within this section of the thesis, the project plan is shown, which is illustrated in figure \ref{fig:Project-Plan}. The project is divided into four different phases. At the end of each phase, there is a milestone, which will review the process within the phase and provide an outlook on the next phase. The first phase is doing research, where many resources are gathered, and essential pieces of information are extracted and documented. The goal of this stage is to get a better understanding of the concepts, which will need to be used, as well as what already has been done by other researchers. Afterwards, the realisation of the project takes place, where the models, input pipeline and Tile2Vec will be implemented and tested. Then different experiments will be conducted, to validate the realisation and experiment with different hyperparameters. At last, the documentation will be finalised and proofread. There is also a one week buffer before the finalisation of the documentation, which will be used for unpredictable problems. The chapters of the documentation will be regularly written during each phase.
\newline
\newline
The figure \ref{fig:Project-Plan} illustrates the project plan, where the phases are shown in orange, tasks within each phase are blue, and milestones are red diamonds. Each dotted column represents one week within a specific month (e.g. Feb, Mar). A task further has information about how far it is into completion.

% set page layout to portrait and save layout for later
\storeareas\defaultpagestyle
\KOMAoptions{pagesize,paper=landscape,DIV=20}
\storeareas\landscapevalues

\begin{figure}
\centering
     \begin{ganttchart}[%Specs
     x unit = 0.9cm,  %<---------------------- New x unit 
     y unit title=0.4cm,
     y unit chart=0.5cm,
     vgrid, hgrid,
     title height=1,
     title label font=\bfseries\footnotesize,
     progress=today,
     today=12,
     today label=Current Week,
     bar/.append style={fill=blue!50},
     group/.append style={draw=black, fill=orange!50},
     milestone/.append style={draw=black, fill=red!50, xscale=0.55}, % 0.5/0.9 â‰ˆ 0.5555
     milestone progress label node/.append style={right=0.2},
     bar incomplete/.append style={fill=none},
     group incomplete/.append style={draw=black,fill=none},
     bar height=0.7,
     group right shift=0,
     group top shift=0.5,
     group height=.3,
     group peaks width={0.2},
     inline]{1}{16}
    %labels

    %\gantttitle{bachelor thesis - deep embedded music}{18}\\
    \gantttitle[]{2020}{16} \\                
    \gantttitle{Feb}{2}
    \gantttitle{Mar}{4}
    \gantttitle{Apr}{4}
    \gantttitle{May}{4}
    \gantttitle{Jun}{2}\\
    
    % 1. Phase: Research
    \ganttgroup[inline=false]{Research}{1}{4}\\ 
    \ganttbar[progress=100,inline=false]{Dataset}{1}{1}\\
    \ganttbar[progress=100,inline=false]{Audio processing}{1}{1}\\
    \ganttbar[progress=100,inline=false]{Triplet loss}{2}{2}\\
    \ganttbar[progress=100,inline=false]{Tile2Vec}{3}{3}\\
    \ganttbar[progress=100,inline=false]{Evaluate research}{4}{4}\\
    \ganttmilestone[inline=false]{Research finished}{4} \\ % M1
    
    % 2. Phase: Realization
    \ganttgroup[inline=false]{Realisation}{5}{8} \\
    \ganttbar[progress=100,inline=false]{Project setup}{5}{5} \\
    \ganttbar[progress=100,inline=false]{Input pipeline}{5}{5} \\
    \ganttbar[progress=100,inline=false]{Default model architectures}{6}{6} \\
    \ganttbar[progress=100,inline=false]{Evaluation workflow}{6}{6} \\
    \ganttbar[progress=100,inline=false]{Unit tests}{6}{6} \\
    \ganttmilestone[inline=false]{Project Setup finished}{6} \\ % M2
    \ganttbar[progress=100,inline=false]{Tile2Vec implementation}{7}{7} \\
    \ganttbar[progress=100,inline=false]{Architecture for experiments}{8}{8} \\
    \ganttmilestone[inline=false]{Realisation finished}{8} \\ % M3
    
    \ganttmilestone[inline=false]{Interim presentation}{9} \\ % M3
    
    % 3. Phase: Experiments
    \ganttgroup[inline=false]{Experiments}{9}{12} \\
    \ganttbar[progress=100,inline=false]{Conduct experiments}{9}{9} \\
    \ganttbar[progress=70,inline=false]{Validate embeddings}{10}{11} \\
    \ganttmilestone[inline=false]{Experiments finished}{11} \\ % M4
    \ganttbar[progress=100,inline=false]{Visualise embeddings}{12}{12} \\
    
    \ganttbar[progress=30,inline=false]{Create pitch video and web abstract}{12}{12} \\
    
    % Buffer
    \ganttgroup[inline=false]{Buffer}{13}{13} \\
    
    % 4. Phase: Documentation
    \ganttgroup[inline=false]{Finalise documentation}{14}{16} \\
    \ganttbar[progress=20,inline=false]{Finalise documentation}{14}{15} \\
    \ganttbar[progress=10,inline=false]{Proofread documentation}{16}{16} \\
    \ganttmilestone[inline=false]{Thesis submission}{16}

\end{ganttchart}
\caption{Project plan}
\label{fig:Project-Plan}
\end{figure}

% set page back to portrait
\clearpage
\defaultpagestyle

\section{Procedure model}
\label{sec:Procedure-Model}
The waterfall model is a breakdown of project activities into linear sequential phases, where each phase depends on the deliverables of the previous one and corresponds to a specialisation of tasks. For this project, a custom waterfall model is used, which differs quite profoundly from Royce's original waterfall model. The procedure model is shown in figure \ref{fig:Project-Plan}.
\newline
\newline
The project is grouped into 4 phases, research, realisation, experiments and finalise documentation. All of these phases consist of multiple tasks which need to be completed before going on the next one.

\subsection{Milestone planning}
\label{sec:Milestone-Planning}
There is a specific milestone for each finished phase, to review the process within the finished phase and provide an outlook on the next phase. This is done by writing a report for each milestone, which can be found in the appendix milestone reports (\ref{app:Milestone-Reports}). Each of these reports gives insight about the status of the project like \textit{what was done since the last reporting}, \textit{state of the progress of the project} and \textit{top three risks}, including planned measures. The reports provide valuable insights into the projects current status and are used to plan the next phases. Table \ref{tab:Milestones} shows all the milestones with the corresponding data assigned to it. The milestones are illustrated in the full project plan \ref{fig:Project-Plan} in red.
\newline
\newline
It is important to note, that the milestone M4: Interim presentation does not conclude a phase. It is purely used to show wherein the project the interim presentation is held and what is already done until then. Therefore, for this milestone, no milestone report is being written.

\begin{table}[htbp]
    \centering
    \caption{Milestones overview along with their corresponding date}
	\label{tab:Milestones}
    \begin{tabular}{p{.70\textwidth} | p{.20\textwidth}}
        \toprule
        \textbf{Milestone} & \textbf{Date} \\ 
        \midrule[1pt]
        M0: Project start & 17.02.2020\\
        \hline
        M1: Research finished & 15.03.2020\\
        \hline
        M2: Project setup finished & 29.03.2020\\
        \hline
        M3: Realisation finished & 12.04.2020\\
        \hline
        M4: Interim presentation & 21.04.2020\\
        \hline
        M5: Experiments finished & 03.05.2020\\
        \hline
        M6: Thesis submission & 05.06.2020\\
        \bottomrule
    \end{tabular}
\end{table}

\section{Evaluation}
\label{sec:Evaluation}
The evaluation is done for both used datasets separately, and in a relatively different way, therefore both evaluations are described in different subsections, the evaluation of the DCASE dataset (\ref{sub:Eval-DCASE}) and the evaluation of the music dataset (\ref{sub:Eval-Music}).

\subsection{DCASE 2018 - Task 5 dataset}
\label{sub:Eval-DCASE}
The evaluation of the embedding space for the \fullref{sub:DCASE-Task-Dataset} is done in a few separate steps. The development dataset of the challenge is further split into an \texttt{development-training} and an \texttt{development-evaluation} set. First, an arbitrary embedding architecture is being trained on the \texttt{development-training} dataset and evaluated on the \texttt{development-evaluation} set using the metrics available for the embedding (\ref{sec:Metrics}), this process is repeated until the desired architecture is found.
\newline
\newline
To then evaluate the performance of the resulting embedding space, a simple classifier is trained using the embeddings as input. The simple classifier consists of a two-layered dense model, which is illustrated in figure \ref{fig:Classifier-DCASE-Visualisation}. To evaluate the different embedding spaces, the macro-averaged F1 score of the classifier is compared to other F1 scores from the experiment. To show the performance gain when using the embedding space, the classifier is trained on the raw audio representation itself to provide a baseline F1 score.
\newline
\newline
In the final step, the resulting embedding space, as well as the trained classifier will be used on the evaluation dataset of the challenge, where the resulting macro-averaged F1 scores are compared to the accomplished score by the submitted models. This aims to show the score which can be accomplished when mitigating the label and using the dataset as an unsupervised one.

\begin{figure}[htbp]
    \captionsetup{format=plain}
    \centering
    \begin{tikzpicture}[start chain=going below, node distance=15pt,
        point/.append style={on chain, join=by {signal}},
        layer/.append style={on chain, join=by {signal}}]
        \node[point] {Input to classifier, embedded sample};
        \node[conv] {Dense layer (hidden layer 1): \\ 256 units, ReLU};
        \node[conv] {Dense layer (hidden layer 2): \\ 256 units, ReLU};
        \node[activation] {Dense output: \\ 9 units (num. of classes), softmax};
        \node[point] {Output};
    \end{tikzpicture}
    \caption{Visualisation of the classifier architecture}
    \label{fig:Classifier-DCASE-Visualisation}
\end{figure}
\noindent
The overall goal is to achieve the highest possible macro-averaged F1 score on the evaluation dataset provided by the DCASE challenge.

\subsection{Music dataset}
\label{sub:Eval-Music}
The process of evaluating the embedding space for the music dataset (\ref{sub:Music-Dataset}) is a bit more complicated since there are no prior results on this exact dataset, neither a baseline model to compare it to. The first evaluation of the embedding is relatively similar to the one for the \fullref{sub:Eval-DCASE}. The dataset is split into an \texttt{training}, \texttt{evaluation} and \texttt{test} set. Then the \texttt{training} dataset is used to train the embedding architecture and is evaluated on the \texttt{evaluation} set with the metrics specified in metrics (\ref{sec:Metrics}). After that, the model, which resulted in the best metrics, is chosen and is further evaluated.
\newline
\newline
Since one of the requirements of the project is to examine the resulting clusters of the music embedding space, a simple clustering algorithm, such as \fullref{sub:K-Means}, is being applied on the resulting embeddings of the \texttt{test} set. This should provide a crucial insight into the resulting clusters and the performance of the embedding architecture.
\newline
\newline
Another requirement of the embedding space is that it should provide some similarity measure for the embedded songs. Therefore the resulting clusters, as well as their distances from each other, are important evaluation criteria. Since the examination of these distances as well as the similarity between each music genre is quite hard, Emanuel Oehri, who provided the music dataset (\ref{sub:Music-Dataset}), is examining the distances between the clusters and gives feedback about their similarity from his point of view. He will also examine the resulting similarity between two songs of different genres which are projected into their near-by region.

\section{Testing}
\label{sec:Testing}
All of the components in the project are tested with the TensorFlow unit tests module called \texttt{tf.test}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/test}}. A test environment is created to provide the necessary datasets for the project, but in a much smaller size to speed up the testing process, since testing on the entire set would be infeasible. All of the test cases for the components are kept as small as possible, but a lot of them require other dependencies, such as the input pipeline, to conduct the test which is fine, because the project is a research project and would need to go through an extensive testing phase before it could be used in a productive environment.
\newline
\newline
The complete test concept, which contains each of the test cases along with the test status, can be found in the appendix test concept (\ref{tab:Test-Concept}). All of the unit tests can be found in the source repository of this project in the \texttt{test} directory.

\begin{table}[htbp]
    \centering
    \caption{Testing method of each project component}
	\label{tab:Components-Testing}
    \begin{tabular}{p{.15\textwidth} | p{.20\textwidth} | p{.60\textwidth}}
        \toprule
        \textbf{Component} & \textbf{Testing method} & \textbf{Reason} \\ 
        \midrule[1pt]
        Feature extractor & unit test & result can be validated, since result is deterministic \\
        \hline
        Datasets & unit test & result can be validated, since result is deterministic \\
        \hline
        Input pipeline & unit test & result can be validated, since result is deterministic \\
        \hline
        Embedding models & unit test + manual testing & automated tests check if the model can be built and outputs values if the model trains can only be checked manually by observing the metrics \\
        \hline
        Classifier models & unit test + manual testing & automated tests check if the model can be built and outputs values if the model trains can only be checked manually by observing the metrics \\
        \hline
        Triplet loss  & unit test & result can be validated, since result is deterministic \\
        \hline
        Training & manual testing & result is not deterministic and can therefore only be tested manually by looking at the metrics over the training process \\
        \hline
        Utils & unit test & result can be validated, since result is deterministic \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Project structure}
\label{sec:Project-Structure}
The thesis consists of two projects, a project for the documentation and a project for the source of the thesis. Both projects are git repositories hosted on GitHub\footnote{\url{https://github.com/}}. For the duration of the thesis, both repositories are kept private, and after the completion of the project will be open-sourced.

\begin{figure}[ht]
    \dirtree{%
    .1 deep-embedded-music/.
    .2 data/ \ldots{} (contains the datasets used).
    .2 experiments/.
    .3 config/ \ldots{} (configuration for the experiment).
    .3 DCASE/ \ldots{} (results from DCASE experiments).
    .3 DJ/ \ldots{} (results from DJ experiments).
    .2 notebooks/ \ldots{} (contains the jupyter-notebooks).
    .2 src/ \ldots{} (source code of the thesis).
    .2 test/ \ldots{} (contains the unit tests).
    .2 test-environment/ \ldots{} (test environment for the unit tests).
    .2 Dockerfile \ldots{} (docker file for the repository).
    .2 onstart.sh \ldots{} (script to start the training).
    .2 requirements.txt \ldots{} (requirements for the project).
    }
\caption{Overview of the source repository \flqq deep embedded music\frqq}
\label{fig:Project-Overview-Source}
\end{figure}
\noindent
Figure \ref{fig:Project-Overview-Source} illustrates the project structure of the source repository, which can be found on GitHub\footnote{\url{https://github.com/FabianGroeger96/deep-embedded-music}}. The illustration shows an overview of the overall structure of the repository. More in-depth explanation can be found in section \ref{ch:Realisation}.

\begin{figure}[ht]
    \dirtree{%
    .1 baa-doc-deep-embedded-music/.
    .2 baa-documentation/.
    .2 baa-intermediate-presentation/.
    .2 baa-final-presentation/.
    .2 baa-study-doc/.
    }
\caption{Overview of the documentation repository \flqq baa-doc-deep-embedded-music\frqq}
\label{fig:Project-Overview-Documentation}
\end{figure}
\noindent
Figure \ref{fig:Project-Overview-Documentation} illustrates the project structure of the repository used for the documentation, which can be found on GitHub\footnote{\url{https://github.com/FabianGroeger96/baa-doc-deep-embedded-music}}. The repository contains a folder for the \textit{documentation}, \textit{interim presentation}, \textit{final presentation} and the \textit{study doc}. The study doc contains detailed information about the conducted experiments, which are attached in the appendix study doc (\ref{app:Study-Doc}).
\newline
\newline
Both datasets used for the thesis were used as they are and therefore did not need to be kept within a specific repository for the project. The dataset of the DCASE challenge 2018 - Task 5 consists of a development and evaluation dataset, which are both hosted on Zendo\footnote{\url{https://zenodo.org}}. The development dataset is available under \url{https://zenodo.org/record/1247102} while the evaluation dataset is available under \url{https://zenodo.org/record/1964758}. 
\newline
\newline
The music dataset was kindly provided by mister Emanuel Oehri and is available as a private GitLab Git LFS repository, which will be kept as a private repository since all the songs provided are property of mister Oehri.

\section{Resources}
\label{sec:Resources}
Deep neural networks architectures are computationally expensive what results in long-running experiments. Using powerful \glspl{GPU} reduces the processing time up to a factor of 20. The Lucerne University of Applied Science and Arts supported this thesis with a GPU GTX 1080Ti, 11 GB RAM, hosted within the enterprise lab\footnote{\url{https://www.enterpriselab.ch/}}. 
\newline
\newline
Additionally, \glspl{GPU} were rented on vast.ai\footnote{\url{https://vast.ai/}} during specific experiments to decrease the training time, by running experiments in parallel. A Tesla V100 \gls{GPU} 16 GB RAM was mostly rented to mitigate the problem of not having enough RAM on the GPU for experiments with large models (e.g. such as ResNet50).

