\chapter{Ideas and concepts}
\label{ch:Ideas-Concepts}

This chapter explains the different ideas and concepts used in the thesis. Some ideas were discovered during the research and were already introduced in the chapter \fullref{ch:Related-Work}. Others were developed especially for this project and their application, as well as their value, will be proven in this work.
\newline
\newline
It is important to note, that in the thesis two distinct datasets are used. Due to that, many of the sections in this chapter are divided into two groups, one for the \fullref{sub:DCASE-Task-Dataset} (noise detection) and one for the \fullref{sub:Music-Dataset}.

\section{Data preprocessing}
\label{sec:Data-Preprocessing}
Data preprocessing is the process of processing the entire dataset before it is fed into the \fullref{sec:Input-Pipeline}, which will be introduced in the next section. Both datasets used in the project contain audio files for each label. It was decided that data preprocessing was not necessary for this project as the data was already well prepared. Both datasets were used as-is.
\newline
\newline
For the \fullref{sub:DCASE-Task-Dataset}, the data was already preprocessed by the organisers of the challenge. This means that there are no audio files which contain multiple labels, are corrupted or invalid. Because of that, no preprocessing of the data was necessary.
\newline
\newline
For the \fullref{sub:Music-Dataset}, the data consists of different songs grouped by their respective genre. Because the audios only contain the songs without any background noise in it, the preprocessing was also neglected.
\newline
\newline
The \fullref{sec:Triplet-Selection} and \fullref{sec:Feature-Extraction} of the audio files is done on the fly within the \fullref{sec:Input-Pipeline}. The advantage of this method is that the pipeline can be adapted fairly quick for other datasets without any preprocessing of the data needed. This is also a big benefit if the project will be needed in the context of real-time audio processing because the raw waveform can be processed directly within the application.

\section{Feature extraction}
\label{sec:Feature-Extraction}
Feature extraction is the process of extracting the relevant features from the raw data; in this case, audio files. This process will be done within the \fullref{sec:Input-Pipeline}. For both datasets, the same feature extractions were used. In this project, three different extractions are evaluated, and are used as a special kind of hyperparameter which can be tuned to achieve the best possible outcome.
\newline
\newline
The first approach is to use the audio files as-is, more precisely to use the raw waveform as input to the model. This is the simplest approach and also the most lightweight to calculate. It will be used to evaluate how good a model can be trained without any feature extraction in the audio domain. The approach to use the raw waveform would also be the simplest one to implement in a real-world application, because there is no complex computation needed, and can therefore also be used on small devices, such as mobile phones or Raspberry Pis.
\newline
\newline
The second approach is to calculate the log-mel spectrogram, which is outlined in subsection \fullref{sub:Mel-Spectrogram} were the last step of calculating the \gls{DCT} is omitted. The process described was introduced to mimic the human hearing, but the last step, calculating the \gls{DCT}, was proposed because many machine learning models struggled with the highly correlated data, such as the log-mel spectrogram, and thus a linear transformation (\gls{DCT}) was added to decorrelate the feature. Contemporary machine learning models such as CNNs or GRUs do not struggle with correlated data und perform even better when they can decorrelate the features themselves. Therefore this approach should give useful insight, weather the decorrelation can be trained and is still needed in present models.
\newline
This approach is also widely used in the audio domain because it is less computing extensive than calculating the \gls{MFCC} and it can achieve almost the same accuracy as a model trained with \gls{MFCC} as their features because it uses the same computations and additionally preserves more information. Thus log-mel spectrograms are the most promising approach for this project.
\newline
\newline
The third and final approach is to calculate the \gls{MFCC}, which is described in subsection \fullref{sub:Mel-Spectrogram}. This feature extraction method is used mainly in extensive audio applications, such as automatic speech and speaker recognition, where a lot of computing power is available. These features are mostly used because of their nature, that they represent sound like the human auditory system does in a compact matter.
\newline
\newline
All of the features stated above are two-dimensional data, because of that nature, standard image processing architectures can be adapted to audio processing.

\section{Data augmentation}
\label{sec:Data-Augmentation}
Data augmentation is the process of changing the real data within the dataset a particular way so that it generates new data. Data augmentation is mainly used to balance imbalanced datasets, where a particular class has significantly less data than others. It can also be used to make the model more invariant to changes in the data so that the model generalises better.
\newline
\newline
Data augmentation will only be used for the \fullref{sub:DCASE-Task-Dataset}, because it is unbalanced in the amount of data each class has, which could correspond to the frequency of the activities in real life. The amount of data in the following six classes: cooking, dishwashing, eating, other, social activity, and vacuum cleaning, is extremely small compared to the other three classes: absence, watching TV and working. Therefore, the amount of audio data for the six classes is increased using data augmentation techniques, proposed in the next paragraph, to mitigate the unbalancing issue. Both techniques are described in further detail within the paper \cite{inoue_domestic_2018}. % TODO insert image of unbalanced dataset
\newline
\newline
The augmentation of the audio data is based on two assumptions. First, it is assumed that the acoustic environment does not depend on the order of sound events. For example, consider a recorded audio file within the class \textit{eating}. Some of the sounds correspond to the sound of events made by kitchen utensils and some to the process of eating. Even if the order of these sound events is swapped, the new sound can still be categorised as \textit{eating}. Therefore new data can be generated by changing the order of sound events. The second assumption is that mixing two sounds in the same class generates new sound within the same class. This assumption has also been used in previous works and has proven to be a beneficial data augmentation technique for audio.\footnote{\fullcite{zhang_mixup_2018}}\footnote{\fullcite{takahashi_deep_2016}}
\newline
\newline
For the \fullref{sub:Music-Dataset}, the data augmentation was neglected because the dataset is not imbalanced and extensive enough. There are precisely 30 songs in each class which indicates a relatively equally distributed dataset.
% TODO insert illustration of data augmentation

\section{Triplet selection}
\label{sec:Triplet-Selection}

The triplet selection describes the process of selecting a neighbouring and opposite feature from a given anchor feature. It is described in further detail within section \fullref{eq:Triplet-Loss}. The procedure is split into two parts, the selection of the neighbouring and opposite feature. This process is relatively different for the two datasets and therefore will be described for each one separately.
\newline
\newline
For the \fullref{sub:DCASE-Task-Dataset} the triplet selection is made by filtering the existing features by a specific description of the entry. The neighbour will be chosen so that the anchor and the neighbour belong to the same class, but belong to different sessions and nodes. The opposite will be chosen so that the anchor and the opposite belong to different classes.
\newline
\newline
For the \fullref{sub:Music-Dataset} the triplet selection is also made by filtering the existing features by a specific description of the entry. The neighbour will be chosen so that the anchor and the neighbour belong to the same class, but belong to different songs. The opposite will be chosen so that the anchor and the opposite belong to different classes.
\newline
\newline
Both methods throw an exception when no corresponding neighbour or opposite feature can be found.

\section{Input pipeline}
\label{sec:Input-Pipeline}
The input pipeline is responsible for creating the dataset, which will then be fed into the model. For this project, the input pipeline is implemented using the Tensorflow \texttt{tf.data} API\footnotemark, along with a generator, which loops over the whole dataset. A generator function is used because the entire dataset can not be loaded into memory, due to the large size of it.
\footnotetext{\url{https://www.tensorflow.org/api_docs/python/tf/data}}
\newline
\newline
Because no preprocessing is being done to the datasets and they are used as-is, the input pipeline will need to do the \fullref{sec:Feature-Extraction}, the \fullref{sec:Data-Augmentation} and the \fullref{sec:Triplet-Selection} as well. Therefore the input pipeline is divided into these sections in the order they occur.
\newline
\newline
The generator function loops over the whole dataset and creates a triplet from the current entry, which will then be yielded back to the dataset. Within the generation of the triplets, the corresponding audios will be loaded from the files. The generation of triplets will be implemented for each dataset separately since the triplet selection is different. It is important to note that this function returns an array of triplets. Thus the \fullref{sec:Data-Augmentation} for the \nameref{sub:DCASE-Task-Dataset} and the slicing of each song for the \nameref{sub:Music-Dataset} can be done within this step. Hence the method yield a triplet of audios and their corresponding labels.
\newline
\newline
This creates the dataset dynamically because the dataset only contains the current batch along with some prefetched entries, which are entries from the next batch so that the \gls{GPU} is fully utilised.
\newline
\newline
The feature extraction is implemented using the vectorised \texttt{.map} function of the class \texttt{tf.data.Dataset}\footnotemark. The \texttt{.map} function applies a given function to each element within the dataset, which in our case will be used to extract the features from the audio. Hence a \flqq feature extractor\frqq \ has to be provided to the input pipeline, which will extract a certain feature from the audio as stated in \fullref{sec:Feature-Extraction}.
\footnotetext{\url{https://www.tensorflow.org/api_docs/python/tf/data/Dataset}}
% TODO insert illustration of process

\section{Models}
\label{sec:Models}

\section{Application to music}
\label{sec:Application-Music}
